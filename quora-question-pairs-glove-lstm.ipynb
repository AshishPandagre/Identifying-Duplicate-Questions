{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-21T03:58:13.757090Z","iopub.execute_input":"2022-08-21T03:58:13.757377Z","iopub.status.idle":"2022-08-21T03:58:19.342684Z","shell.execute_reply.started":"2022-08-21T03:58:13.757330Z","shell.execute_reply":"2022-08-21T03:58:19.341665Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Read the train and test dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/quora-question-pairs/train.csv.zip')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-08-21T03:58:19.344567Z","iopub.execute_input":"2022-08-21T03:58:19.344905Z","iopub.status.idle":"2022-08-21T03:58:21.273513Z","shell.execute_reply.started":"2022-08-21T03:58:19.344857Z","shell.execute_reply":"2022-08-21T03:58:21.272512Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:21.275321Z","iopub.execute_input":"2022-08-21T03:58:21.275892Z","iopub.status.idle":"2022-08-21T03:58:21.297009Z","shell.execute_reply.started":"2022-08-21T03:58:21.275837Z","shell.execute_reply":"2022-08-21T03:58:21.295657Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/quora-question-pairs/test.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:21.298944Z","iopub.execute_input":"2022-08-21T03:58:21.299707Z","iopub.status.idle":"2022-08-21T03:58:34.243482Z","shell.execute_reply.started":"2022-08-21T03:58:21.299295Z","shell.execute_reply":"2022-08-21T03:58:34.242473Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.246769Z","iopub.execute_input":"2022-08-21T03:58:34.247083Z","iopub.status.idle":"2022-08-21T03:58:34.257690Z","shell.execute_reply.started":"2022-08-21T03:58:34.247028Z","shell.execute_reply":"2022-08-21T03:58:34.256610Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"This is our test data. We'll calculate the similarity between these question pairs to get the idea whether the are duplicates or not.","metadata":{}},{"cell_type":"code","source":"X_train = df.iloc[:,:5].values\nY_train = df.iloc[:,5:].values","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.260243Z","iopub.execute_input":"2022-08-21T03:58:34.260848Z","iopub.status.idle":"2022-08-21T03:58:34.364687Z","shell.execute_reply.started":"2022-08-21T03:58:34.260751Z","shell.execute_reply":"2022-08-21T03:58:34.363738Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_testq1 = test_data.iloc[:400001,1:2].values\nX_testq2 = test_data.iloc[:400001, 2:].values","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.367485Z","iopub.execute_input":"2022-08-21T03:58:34.368057Z","iopub.status.idle":"2022-08-21T03:58:34.373843Z","shell.execute_reply.started":"2022-08-21T03:58:34.368001Z","shell.execute_reply":"2022-08-21T03:58:34.372891Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"s1 = X_train[:,3]\ns2 = X_train[:,4]","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.376328Z","iopub.execute_input":"2022-08-21T03:58:34.376647Z","iopub.status.idle":"2022-08-21T03:58:34.383944Z","shell.execute_reply.started":"2022-08-21T03:58:34.376599Z","shell.execute_reply":"2022-08-21T03:58:34.383072Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def tokenize(s):\n    tokens = []\n    tokens = [word_tokenize(str(sentence)) for sentence in s]\n\n    rm1 = []\n    for w in tokens:\n        sm = re.sub('[^A-Za-z]',' ', str(w))\n        x = re.split(\"\\s\", sm)\n        rm1.append(x)\n        \n    return rm1","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.385741Z","iopub.execute_input":"2022-08-21T03:58:34.386241Z","iopub.status.idle":"2022-08-21T03:58:34.394265Z","shell.execute_reply.started":"2022-08-21T03:58:34.386065Z","shell.execute_reply":"2022-08-21T03:58:34.393081Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def lower_case(s):\n    #Removing whitespaces    \n    for sent in s:\n        while '' in sent:\n            sent.remove('')\n\n    # Lowercasing\n    low = []\n    for i in s:\n        i = [x.lower() for x in i]\n        low.append(i)\n        \n    return low\n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.396014Z","iopub.execute_input":"2022-08-21T03:58:34.396775Z","iopub.status.idle":"2022-08-21T03:58:34.403197Z","shell.execute_reply.started":"2022-08-21T03:58:34.396586Z","shell.execute_reply":"2022-08-21T03:58:34.402348Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def lemmatize(s):\n    lemma = []\n    wnl = WordNetLemmatizer()\n    for doc in s:\n        tokens = [wnl.lemmatize(w) for w in doc]\n        lemma.append(tokens)\n\n    # Removing Stopwords\n    filter_words = []\n    Stopwords = set(stopwords.words('english'))\n\n    #ab = spell('nd')\n    for sent in lemma:\n        tokens = [w for w in sent if w not in Stopwords]\n        filter_words.append(tokens)\n\n    space = ' ' \n    sentences = []\n    for sentence in filter_words:\n        sentences.append(space.join(sentence))\n        \n    return sentences","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.404516Z","iopub.execute_input":"2022-08-21T03:58:34.405055Z","iopub.status.idle":"2022-08-21T03:58:34.414925Z","shell.execute_reply.started":"2022-08-21T03:58:34.405006Z","shell.execute_reply":"2022-08-21T03:58:34.414072Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# sent1 = tokenize(s1)\n# sent2 = tokenize(s2)\n# q1 = lower_case(sent1)\n# q2 = lower_case(sent2)\n# listq1 = lemmatize(q1)\n# listq2 = lemmatize(q2)\n# sent1_t = tokenize(X_test_q1)\n# sent2_t = tokenize(X_test_q2)\n# q1_t = lower_case(sent1_t)\n# q2_t = lower_case(sent2_t)\n# listq1 = lemmatize(q1_t)\n# listq2 = lemmatize(q2_t)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.417630Z","iopub.execute_input":"2022-08-21T03:58:34.418299Z","iopub.status.idle":"2022-08-21T03:58:34.423323Z","shell.execute_reply.started":"2022-08-21T03:58:34.418225Z","shell.execute_reply":"2022-08-21T03:58:34.422632Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MAX_NB_WORDS = 200000\ntokenizer = Tokenizer(num_words = MAX_NB_WORDS)\ntokenizer.fit_on_texts(list(df['question1'].values.astype(str))+list(df['question2'].values.astype(str)))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:34.424909Z","iopub.execute_input":"2022-08-21T03:58:34.425450Z","iopub.status.idle":"2022-08-21T03:58:54.359712Z","shell.execute_reply.started":"2022-08-21T03:58:34.425264Z","shell.execute_reply":"2022-08-21T03:58:54.358665Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# X_train_q1 = tokenizer.texts_to_sequences(np.array(listq1))\nX_train_q1 = tokenizer.texts_to_sequences(df['question1'].values.astype(str))\nX_train_q1 = pad_sequences(X_train_q1, maxlen = 30, padding='post')\n\n# X_train_q2 = tokenizer.texts_to_sequences(np.array(listq2))\nX_train_q2 = tokenizer.texts_to_sequences(df['question2'].values.astype(str))\nX_train_q2 = pad_sequences(X_train_q2, maxlen = 30, padding='post')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:58:54.361062Z","iopub.execute_input":"2022-08-21T03:58:54.361411Z","iopub.status.idle":"2022-08-21T03:59:16.610672Z","shell.execute_reply.started":"2022-08-21T03:58:54.361356Z","shell.execute_reply":"2022-08-21T03:59:16.609624Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_test_q1 = tokenizer.texts_to_sequences(X_testq1.ravel())\nX_test_q1 = pad_sequences(X_test_q1,maxlen = 30, padding='post')\n\nX_test_q2 = tokenizer.texts_to_sequences(X_testq2.astype(str).ravel())\nX_test_q2 = pad_sequences(X_test_q2, maxlen = 30, padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:59:16.611972Z","iopub.execute_input":"2022-08-21T03:59:16.612335Z","iopub.status.idle":"2022-08-21T03:59:36.337565Z","shell.execute_reply.started":"2022-08-21T03:59:16.612284Z","shell.execute_reply":"2022-08-21T03:59:36.336594Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:59:36.338882Z","iopub.execute_input":"2022-08-21T03:59:36.339239Z","iopub.status.idle":"2022-08-21T03:59:36.344154Z","shell.execute_reply.started":"2022-08-21T03:59:36.339184Z","shell.execute_reply":"2022-08-21T03:59:36.343227Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"embedding_index = {}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_index[word] = vectors\n    f.close()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T03:59:36.345470Z","iopub.execute_input":"2022-08-21T03:59:36.346023Z","iopub.status.idle":"2022-08-21T04:00:00.471349Z","shell.execute_reply.started":"2022-08-21T03:59:36.345973Z","shell.execute_reply":"2022-08-21T04:00:00.470518Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.random.random((len(word_index)+1, 200))\nfor word, i in word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-08-21T04:00:00.474915Z","iopub.execute_input":"2022-08-21T04:00:00.475204Z","iopub.status.idle":"2022-08-21T04:00:00.768452Z","shell.execute_reply.started":"2022-08-21T04:00:00.475156Z","shell.execute_reply":"2022-08-21T04:00:00.767656Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Model for Q1\nimport tensorflow as tf\nfrom tensorflow.keras.layers import BatchNormalization\nmodel_q1 = tf.keras.Sequential()\nmodel_q1.add(Embedding(input_dim = len(word_index)+1,\n                       output_dim = 200,\n                      weights = [embedding_matrix],\n                      input_length = 30))\nmodel_q1.add(LSTM(128, activation = 'tanh', return_sequences = True))\nmodel_q1.add(Dropout(0.2))\nmodel_q1.add(LSTM(128, return_sequences = True))\nmodel_q1.add(LSTM(128))\nmodel_q1.add(Dense(60, activation = 'tanh'))\nmodel_q1.add(Dense(2, activation = 'sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T04:00:00.771412Z","iopub.execute_input":"2022-08-21T04:00:00.771814Z","iopub.status.idle":"2022-08-21T04:00:04.200175Z","shell.execute_reply.started":"2022-08-21T04:00:00.771756Z","shell.execute_reply":"2022-08-21T04:00:04.199264Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Model for Q2\nmodel_q2 = tf.keras.Sequential()\nmodel_q2.add(Embedding(input_dim = len(word_index)+1,\n                       output_dim = 200,\n                      weights = [embedding_matrix],\n                      input_length = 30))\nmodel_q2.add(LSTM(128, activation = 'tanh', return_sequences = True))\nmodel_q2.add(Dropout(0.2))\nmodel_q2.add(LSTM(128, return_sequences = True))\nmodel_q2.add(LSTM(128))\nmodel_q2.add(Dense(60, activation = 'tanh'))\nmodel_q2.add(Dense(2, activation = 'sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T04:00:04.201494Z","iopub.execute_input":"2022-08-21T04:00:04.201856Z","iopub.status.idle":"2022-08-21T04:00:05.070133Z","shell.execute_reply.started":"2022-08-21T04:00:04.201809Z","shell.execute_reply":"2022-08-21T04:00:05.069333Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Merging the output of the two models,i.e, model_q1 and model_q2\nmergedOut = Multiply()([model_q1.output, model_q2.output])\n\nmergedOut = Flatten()(mergedOut)\nmergedOut = Dense(100, activation = 'relu')(mergedOut)\nmergedOut = Dropout(0.2)(mergedOut)\nmergedOut = Dense(50, activation = 'relu')(mergedOut)\nmergedOut = Dropout(0.2)(mergedOut)\nmergedOut = Dense(2, activation = 'sigmoid')(mergedOut)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T04:00:05.071459Z","iopub.execute_input":"2022-08-21T04:00:05.071812Z","iopub.status.idle":"2022-08-21T04:00:05.130049Z","shell.execute_reply.started":"2022-08-21T04:00:05.071764Z","shell.execute_reply":"2022-08-21T04:00:05.129345Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"new_model = Model([model_q1.input, model_q2.input], mergedOut)\nnew_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',\n                 metrics = ['accuracy'])\nhistory = new_model.fit([X_train_q1,X_train_q2],Y_train, batch_size = 2000, epochs = 10)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T04:00:05.131257Z","iopub.execute_input":"2022-08-21T04:00:05.131669Z","iopub.status.idle":"2022-08-21T04:23:19.781513Z","shell.execute_reply.started":"2022-08-21T04:00:05.131572Z","shell.execute_reply":"2022-08-21T04:23:19.780606Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"y_pred = new_model.predict([X_test_q1, X_test_q2], batch_size=2000, verbose=1)\ny_pred += new_model.predict([X_test_q1, X_test_q2], batch_size=2000, verbose=1)\ny_pred /= 2","metadata":{"execution":{"iopub.status.busy":"2022-08-21T04:23:19.782939Z","iopub.execute_input":"2022-08-21T04:23:19.783307Z","iopub.status.idle":"2022-08-21T04:23:39.515354Z","shell.execute_reply.started":"2022-08-21T04:23:19.783257Z","shell.execute_reply":"2022-08-21T04:23:39.514631Z"},"trusted":true},"execution_count":23,"outputs":[]}]}